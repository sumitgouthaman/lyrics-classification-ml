{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sumit/Venv/mlpy3venv/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Python 2 / 3 compatibility\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import layers as kl\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\nAnd...  \n",
       "1  Take it easy with me, please  \\nTouch me gentl...  \n",
       "2  I'll never know why I had to go  \\nWhy I had t...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_pd = pd.read_csv(\"songdata.csv\", )\n",
    "dataset_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl   \n",
       "1   ABBA       Andante, Andante   \n",
       "2   ABBA         As Good As New   \n",
       "3   ABBA                   Bang   \n",
       "4   ABBA       Bang-A-Boomerang   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\nAnd...  \n",
       "1  Take it easy with me, please  \\nTouch me gentl...  \n",
       "2  I'll never know why I had to go  \\nWhy I had t...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del(dataset_pd['link'])\n",
    "dataset_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>57650</td>\n",
       "      <td>57650</td>\n",
       "      <td>57650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>643</td>\n",
       "      <td>44824</td>\n",
       "      <td>57494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Donna Summer</td>\n",
       "      <td>Have Yourself A Merry Little Christmas</td>\n",
       "      <td>I've got sunshine on a cloudy day  \\nWhen it's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>191</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              artist                                    song  \\\n",
       "count          57650                                   57650   \n",
       "unique           643                                   44824   \n",
       "top     Donna Summer  Have Yourself A Merry Little Christmas   \n",
       "freq             191                                      35   \n",
       "\n",
       "                                                     text  \n",
       "count                                               57650  \n",
       "unique                                              57494  \n",
       "top     I've got sunshine on a cloudy day  \\nWhen it's...  \n",
       "freq                                                    6  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_pd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Donna Summer</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Gordon Lightfoot</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Bob Dylan</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>George Strait</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Cher</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               artist  counts\n",
       "103      Donna Summer     191\n",
       "179  Gordon Lightfoot     189\n",
       "40          Bob Dylan     188\n",
       "172     George Strait     188\n",
       "61               Cher     187"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artist_counts = dataset_pd \\\n",
    "    .groupby([\"artist\"]) \\\n",
    "    .size() \\\n",
    "    .reset_index(name=\"counts\") \\\n",
    "    .sort_values(\"counts\", ascending=False)\n",
    "\n",
    "artist_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Exo-K</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>U-Kiss</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>X-Treme</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>Zoe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>Zed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      artist  counts\n",
       "136    Exo-K       2\n",
       "518   U-Kiss       1\n",
       "591  X-Treme       1\n",
       "637      Zoe       1\n",
       "633      Zed       1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artist_counts.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 7.,  6.,  2.,  5.,  6., 11.,  7.,  7., 17.,  6.,  9., 10.,  9.,\n",
       "         4.,  6.,  8.,  9.,  8., 12.,  2.,  9., 12.,  4.,  6., 12.,  8.,\n",
       "         4.,  8.,  5.,  3.,  6.,  8.,  5.,  6.,  8.,  4., 12.,  8.,  8.,\n",
       "         4.,  9.,  2.,  5.,  5., 11.,  6.,  9.,  4., 10.,  2., 11.,  6.,\n",
       "         6.,  9.,  5.,  6.,  8.,  8., 11.,  2.,  5.,  5.,  4.,  7.,  1.,\n",
       "         5.,  4.,  2.,  3.,  2.,  2.,  6.,  5.,  6.,  5.,  8.,  8., 10.,\n",
       "         6.,  3.,  4.,  6., 12.,  7.,  5., 11.,  5., 10.,  8.,  2.,  8.,\n",
       "         9.,  6.,  5.,  3.,  3.,  6.,  6.,  3.,  1.]),\n",
       " array([  1. ,   2.9,   4.8,   6.7,   8.6,  10.5,  12.4,  14.3,  16.2,\n",
       "         18.1,  20. ,  21.9,  23.8,  25.7,  27.6,  29.5,  31.4,  33.3,\n",
       "         35.2,  37.1,  39. ,  40.9,  42.8,  44.7,  46.6,  48.5,  50.4,\n",
       "         52.3,  54.2,  56.1,  58. ,  59.9,  61.8,  63.7,  65.6,  67.5,\n",
       "         69.4,  71.3,  73.2,  75.1,  77. ,  78.9,  80.8,  82.7,  84.6,\n",
       "         86.5,  88.4,  90.3,  92.2,  94.1,  96. ,  97.9,  99.8, 101.7,\n",
       "        103.6, 105.5, 107.4, 109.3, 111.2, 113.1, 115. , 116.9, 118.8,\n",
       "        120.7, 122.6, 124.5, 126.4, 128.3, 130.2, 132.1, 134. , 135.9,\n",
       "        137.8, 139.7, 141.6, 143.5, 145.4, 147.3, 149.2, 151.1, 153. ,\n",
       "        154.9, 156.8, 158.7, 160.6, 162.5, 164.4, 166.3, 168.2, 170.1,\n",
       "        172. , 173.9, 175.8, 177.7, 179.6, 181.5, 183.4, 185.3, 187.2,\n",
       "        189.1, 191. ]),\n",
       " <a list of 100 Patch objects>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd3e49ea630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(artist_counts[\"counts\"], bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_search(bands, artist_counts):\n",
    "    lower_map = {a.lower(): a for a in artist_counts[\"artist\"].values}\n",
    "    results = []\n",
    "    bands = [b.lower() for b in bands]\n",
    "    artist_names = lower_map.keys()\n",
    "    for b in bands:\n",
    "        for a in artist_names:\n",
    "            if (b in a) and (len(b) / len(a) > 0.6):\n",
    "                actual_name = lower_map[a]\n",
    "                count = artist_counts[\"counts\"][artist_counts[\"artist\"] == actual_name]\n",
    "                print(\"Found %s as %s, Count: %d\" % (b, actual_name, count))\n",
    "                results.append(actual_name)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found beatles as The Beatles, Count: 178\n",
      "Found abba as ABBA, Count: 113\n",
      "Found rolling stones as Rolling Stones, Count: 179\n",
      "Found beach boys as Beach Boys, Count: 151\n",
      "Found eagles as Eagles, Count: 41\n",
      "Found coldplay as Coldplay, Count: 120\n",
      "['The Beatles', 'ABBA', 'Rolling Stones', 'Beach Boys', 'Eagles', 'Coldplay']\n"
     ]
    }
   ],
   "source": [
    "favorite_bands = [\"Beatles\", \"ABBA\", \"Rolling Stones\", \"Beach Boys\", \"Eagles\", \"AC/DC\", \"Coldplay\", \"Led Zeppelin\"]\n",
    "BANDS = fuzzy_search(favorite_bands, artist_counts)\n",
    "print(BANDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANDS.remove(\"Eagles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl   \n",
       "1   ABBA       Andante, Andante   \n",
       "2   ABBA         As Good As New   \n",
       "3   ABBA                   Bang   \n",
       "4   ABBA       Bang-A-Boomerang   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\nAnd...  \n",
       "1  Take it easy with me, please  \\nTouch me gentl...  \n",
       "2  I'll never know why I had to go  \\nWhy I had t...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_filtered = dataset_pd[dataset_pd[\"artist\"].isin(BANDS)]\n",
    "dataset_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle\n",
    "dataset_filtered = dataset_filtered.sample(frac=1)\n",
    "lyrics_X = dataset_filtered[\"text\"].values\n",
    "artist_Y = dataset_filtered[\"artist\"].values\n",
    "\n",
    "DATASET_SIZE = len(lyrics_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Beatles' 'Coldplay' 'Beach Boys']\n"
     ]
    }
   ],
   "source": [
    "print(artist_Y[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well, how come you say you will when you won't?  \n",
      "Say you do, baby when you don't  \n",
      "Let me know, honey, how you fell  \n",
      "Tell the truth now, is love real?  \n",
      "So, aw, aw  \n",
      "  \n",
      "Well, honey don't  \n",
      "Well, hon\n",
      "-------------\n",
      "Was a long and dark December  \n",
      "From the rooftops I remember  \n",
      "There was snow  \n",
      "White snow  \n",
      "  \n",
      "Clearly I remember  \n",
      "From the windows they were watching  \n",
      "While we froze down below  \n",
      "  \n",
      "When the future\n",
      "-------------\n",
      "We knew it must have been late  \n",
      "(Tick-tock  \n",
      "Tick-tock)  \n",
      "We had no time to wait  \n",
      "(Tick-tock  \n",
      "Tick-tock)  \n",
      "I went to light the fireplace  \n",
      "(Tick-tock  \n",
      "Tick-tock)  \n",
      "I planned it all this way  \n",
      "And \n",
      "['The Beatles' 'Coldplay' 'Beach Boys']\n",
      "{0: 'The Beatles', 1: 'ABBA', 2: 'Rolling Stones', 3: 'Beach Boys', 4: 'Coldplay'}\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "bands_to_index = {b: i for (i, b) in enumerate(BANDS)}\n",
    "index_to_bands = {i: b for b, i in bands_to_index.items()}\n",
    "\n",
    "artist_numeric_Y = [bands_to_index[a] for a in artist_Y]\n",
    "artist_onehot_Y = to_categorical(artist_numeric_Y)\n",
    "\n",
    "print(\"\\n-------------\\n\".join(map(lambda l: l[:200], lyrics_X[:3])))\n",
    "print(artist_Y[:3])\n",
    "print(index_to_bands)\n",
    "print(artist_onehot_Y[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(741,)\n",
      "(741, 5)\n"
     ]
    }
   ],
   "source": [
    "print(lyrics_X.shape)\n",
    "print(artist_onehot_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max lyrics word length: 611\n",
      "Unique words: 6741\n"
     ]
    }
   ],
   "source": [
    "split_lyrics = []\n",
    "max_lyrics_word_len = 0\n",
    "for l in lyrics_X:\n",
    "    split_l = text_to_word_sequence(l)\n",
    "    max_lyrics_word_len = max(max_lyrics_word_len, len(split_l))\n",
    "    split_lyrics.extend(split_l)\n",
    "\n",
    "unique_words_set = set(split_lyrics)\n",
    "\n",
    "word_counts = {}\n",
    "for w in unique_words_set:\n",
    "    word_counts[w] = 0\n",
    "    \n",
    "for w in split_lyrics:\n",
    "    word_counts[w] = word_counts[w] + 1\n",
    "\n",
    "print('Max lyrics word length: %d' % max_lyrics_word_len)\n",
    "print('Unique words: %d' % len(unique_words_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd3e411c470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts_arr = sorted(list(word_counts.values()), reverse=True)\n",
    "plt.plot(counts_arr)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(datasetX_lyrics, max_words, max_len):\n",
    "    print ('Processing dataset, max_words: %d, max_length: %d' % (max_words, max_len))\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(datasetX_lyrics)\n",
    "\n",
    "    datasetX_tokenized = tokenizer.texts_to_sequences(datasetX_lyrics)\n",
    "    datasetX_padded = pad_sequences(datasetX_tokenized, maxlen=max_len)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens in dataset.' % len(word_index))\n",
    "    \n",
    "    return datasetX_padded, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset, max_words: 6000, max_length: 611\n",
      "Found 6741 unique tokens in dataset.\n",
      "--------------------------------------------------\n",
      "Well, how come you say you will when you won't?  \n",
      "Say you do, baby when you don't  \n",
      "Let me know, honey, how you fell  \n",
      "Tell the truth now, is love real?  \n",
      "So, aw, aw  \n",
      "  \n",
      "Well, honey don't  \n",
      "Well, honey don't  \n",
      "Honey don't  \n",
      "Honey don't  \n",
      "Honey don't  \n",
      "I say you will when you won't  \n",
      "Aw, aw, honey don't  \n",
      "  \n",
      "Well, I love you, baby, and you ought to know  \n",
      "I like the way that you wear your clothes  \n",
      "Everything about you is so doggone sweet  \n",
      "You got that sand all over your feet  \n",
      "So, aw, aw  \n",
      "  \n",
      "Well, honey don't  \n",
      "Honey don't  \n",
      "Honey don't  \n",
      "Honey don't  \n",
      "Honey don't  \n",
      "I say you will when you won't  \n",
      "Aw, aw, honey don't  \n",
      "  \n",
      "I feel fine  \n",
      "Ooo, oo, I say  \n",
      "Well Sometimes I love you on a Saturday night  \n",
      "Sunday morning you don't look right  \n",
      "You been out painting the town  \n",
      "Uh huh, baby you been slipping around  \n",
      "So, aw, aw  \n",
      "  \n",
      "Well, honey don't  \n",
      "I say, honey don't  \n",
      "Honey don't  \n",
      "Honey don't  \n",
      "Honey don't  \n",
      "I say you will when you won't  \n",
      "Aw, aw, honey don't  \n",
      "Well, honey don't  \n",
      "Well, honey don't  \n",
      "A little, little, honey don't  \n",
      "I say, you will when you won't  \n",
      "Aw, aw, honey don't\n",
      "\n",
      "\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0   54   88   41    3   69    3   71   32    3\n",
      "  108   69    3   25   24   32    3   23   57    7   30  150   88    3\n",
      "  770  105    2  728   31   21   19  375   20  944  944   54  150   23\n",
      "   54  150   23  150   23  150   23  150   23    1   69    3   71   32\n",
      "    3  108  944  944  150   23   54    1   19    3   24    6    3 2488\n",
      "    5   30    1   40    2   62   18    3 1137   13  771  175  134    3\n",
      "   21   20 1884  161    3   42   18  859   14  126   13  430   20  944\n",
      "  944   54  150   23  150   23  150   23  150   23  150   23    1   69\n",
      "    3   71   32    3  108  944  944  150   23    1   75  329  171  307\n",
      "    1   69   54  317    1   19    3   11    4 1580   76 1138  269    3\n",
      "   23  123   89    3   91   56 2489    2  352  663  449   24    3   91\n",
      " 1206   97   20  944  944   54  150   23    1   69  150   23  150   23\n",
      "  150   23  150   23    1   69    3   71   32    3  108  944  944  150\n",
      "   23   54  150   23   54  150   23    4   70   70  150   23    1   69\n",
      "    3   71   32    3  108  944  944  150   23]\n",
      "--------------------------------------------------\n",
      "DatasetX padded shape: (741, 611)\n"
     ]
    }
   ],
   "source": [
    "datasetX_padded, _ = process_dataset(lyrics_X, 6000, 611)\n",
    "print('-' * 50)\n",
    "print(lyrics_X[0])\n",
    "print(datasetX_padded[0])\n",
    "print('-' * 50)\n",
    "print('DatasetX padded shape:', datasetX_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  944,  150,   23],\n",
       "       [   0,    0,    0, ...,  131,  353, 1708],\n",
       "       [   0,    0,    0, ...,  318,  664,  664],\n",
       "       [   0,    0,    0, ...,  313,   22,    7],\n",
       "       [   0,    0,    0, ...,   12,    2, 1457]], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetX_padded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoint_callback(model):\n",
    "    dir_name = model.name\n",
    "    if not os.path.isdir(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "    return ModelCheckpoint(\n",
    "        dir_name + '/weights-epoch-{epoch:02d}-vloss-{val_loss:.2f}.hdf5',\n",
    "        monitor='val_loss',\n",
    "        period=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "\n",
    "def get_embedding_layer(tokenizer, embedding_dims, max_words, max_len, trainable=False):\n",
    "    embeddings_index = {}\n",
    "    with open(os.path.join(GLOVE_DIR, 'glove.6B.%dd.txt' % embedding_dims)) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    # prepare embedding matrix\n",
    "    word_index = tokenizer.word_index\n",
    "    num_words = min(max_words, len(word_index))\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dims))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    # load pre-trained word embeddings into an Embedding layer\n",
    "    # note that we set trainable = False so as to keep the embeddings fixed\n",
    "    embedding_layer = kl.Embedding(num_words,\n",
    "                                embedding_dims,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_len,\n",
    "                                trainable=trainable)  \n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset, max_words: 6741, max_length: 611\n",
      "Found 6741 unique tokens in dataset.\n",
      "Found 400000 word vectors.\n",
      "Train on 592 samples, validate on 149 samples\n",
      "Epoch 1/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.6683 - acc: 0.1909 - val_loss: 1.5988 - val_acc: 0.2349\n",
      "Epoch 2/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.6103 - acc: 0.2111 - val_loss: 1.5857 - val_acc: 0.2483\n",
      "Epoch 3/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.5985 - acc: 0.2568 - val_loss: 1.5810 - val_acc: 0.2483\n",
      "Epoch 4/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.6129 - acc: 0.2416 - val_loss: 1.5781 - val_acc: 0.3020\n",
      "Epoch 5/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.5840 - acc: 0.2720 - val_loss: 1.5759 - val_acc: 0.3087\n",
      "Epoch 6/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.5793 - acc: 0.2922 - val_loss: 1.5765 - val_acc: 0.3020\n",
      "Epoch 7/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.5616 - acc: 0.2787 - val_loss: 1.5728 - val_acc: 0.3154\n",
      "Epoch 8/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.5623 - acc: 0.2669 - val_loss: 1.5670 - val_acc: 0.3087\n",
      "Epoch 9/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.5537 - acc: 0.2889 - val_loss: 1.5641 - val_acc: 0.3289\n",
      "Epoch 10/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.5411 - acc: 0.3159 - val_loss: 1.5609 - val_acc: 0.3087\n",
      "Epoch 11/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.5535 - acc: 0.3091 - val_loss: 1.5593 - val_acc: 0.2819\n",
      "Epoch 12/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.5553 - acc: 0.3260 - val_loss: 1.5595 - val_acc: 0.2819\n",
      "Epoch 13/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.5210 - acc: 0.3361 - val_loss: 1.5549 - val_acc: 0.2953\n",
      "Epoch 14/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.5149 - acc: 0.3581 - val_loss: 1.5511 - val_acc: 0.3423\n",
      "Epoch 15/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.5093 - acc: 0.3530 - val_loss: 1.5479 - val_acc: 0.2752\n",
      "Epoch 16/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.4808 - acc: 0.3649 - val_loss: 1.5446 - val_acc: 0.3087\n",
      "Epoch 17/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.5055 - acc: 0.3345 - val_loss: 1.5431 - val_acc: 0.3557\n",
      "Epoch 18/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.5033 - acc: 0.3395 - val_loss: 1.5408 - val_acc: 0.2953\n",
      "Epoch 19/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.4968 - acc: 0.3345 - val_loss: 1.5445 - val_acc: 0.2886\n",
      "Epoch 20/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.4777 - acc: 0.3784 - val_loss: 1.5304 - val_acc: 0.3356\n",
      "Epoch 21/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.4829 - acc: 0.3649 - val_loss: 1.5145 - val_acc: 0.2953\n",
      "Epoch 22/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.4485 - acc: 0.3885 - val_loss: 1.5170 - val_acc: 0.2752\n",
      "Epoch 23/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.4410 - acc: 0.3936 - val_loss: 1.5080 - val_acc: 0.3289\n",
      "Epoch 24/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.4249 - acc: 0.3986 - val_loss: 1.4912 - val_acc: 0.3087\n",
      "Epoch 25/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.4308 - acc: 0.4037 - val_loss: 1.5030 - val_acc: 0.3154\n",
      "Epoch 26/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.4139 - acc: 0.3936 - val_loss: 1.5078 - val_acc: 0.3154\n",
      "Epoch 27/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.3967 - acc: 0.4105 - val_loss: 1.5177 - val_acc: 0.2886\n",
      "Epoch 28/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.4121 - acc: 0.4139 - val_loss: 1.5170 - val_acc: 0.3490\n",
      "Epoch 29/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.3948 - acc: 0.4291 - val_loss: 1.5349 - val_acc: 0.3423\n",
      "Epoch 30/100\n",
      "512/592 [========================>.....] - ETA: 0s - loss: 1.3855 - acc: 0.4414\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.3885 - acc: 0.4375 - val_loss: 1.5389 - val_acc: 0.2752\n",
      "Epoch 31/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.4005 - acc: 0.4003 - val_loss: 1.5155 - val_acc: 0.3020\n",
      "Epoch 32/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.3607 - acc: 0.4493 - val_loss: 1.5167 - val_acc: 0.3020\n",
      "Epoch 33/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.3389 - acc: 0.4561 - val_loss: 1.5057 - val_acc: 0.3154\n",
      "Epoch 34/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.3543 - acc: 0.4375 - val_loss: 1.4892 - val_acc: 0.3557\n",
      "Epoch 35/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.3507 - acc: 0.4274 - val_loss: 1.5154 - val_acc: 0.3490\n",
      "Epoch 36/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.3639 - acc: 0.4274 - val_loss: 1.5218 - val_acc: 0.3289\n",
      "Epoch 37/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.3366 - acc: 0.4578 - val_loss: 1.5145 - val_acc: 0.3154\n",
      "Epoch 38/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.3187 - acc: 0.4392 - val_loss: 1.5137 - val_acc: 0.3087\n",
      "Epoch 39/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.3050 - acc: 0.4443 - val_loss: 1.5109 - val_acc: 0.3423\n",
      "Epoch 40/100\n",
      "512/592 [========================>.....] - ETA: 0s - loss: 1.3348 - acc: 0.4473\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.3140 - acc: 0.4544 - val_loss: 1.5171 - val_acc: 0.3289\n",
      "Epoch 41/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.3045 - acc: 0.4696 - val_loss: 1.5058 - val_acc: 0.3490\n",
      "Epoch 42/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2937 - acc: 0.4797 - val_loss: 1.4992 - val_acc: 0.3490\n",
      "Epoch 43/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2909 - acc: 0.4544 - val_loss: 1.4937 - val_acc: 0.3557\n",
      "Epoch 44/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2703 - acc: 0.4882 - val_loss: 1.4834 - val_acc: 0.3557\n",
      "Epoch 45/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.3036 - acc: 0.4696 - val_loss: 1.5415 - val_acc: 0.3356\n",
      "Epoch 46/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2963 - acc: 0.4679 - val_loss: 1.4963 - val_acc: 0.3356\n",
      "Epoch 47/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.3010 - acc: 0.4628 - val_loss: 1.4926 - val_acc: 0.3289\n",
      "Epoch 48/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2761 - acc: 0.4780 - val_loss: 1.4827 - val_acc: 0.3221\n",
      "Epoch 49/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.3079 - acc: 0.4645 - val_loss: 1.5044 - val_acc: 0.3356\n",
      "Epoch 50/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2764 - acc: 0.4679 - val_loss: 1.4968 - val_acc: 0.3490\n",
      "Epoch 51/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2642 - acc: 0.4882 - val_loss: 1.4912 - val_acc: 0.3423\n",
      "Epoch 52/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.3017 - acc: 0.4459 - val_loss: 1.5051 - val_acc: 0.3356\n",
      "Epoch 53/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2547 - acc: 0.4797 - val_loss: 1.5354 - val_acc: 0.3154\n",
      "Epoch 54/100\n",
      "512/592 [========================>.....] - ETA: 0s - loss: 1.2735 - acc: 0.4668\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2714 - acc: 0.4747 - val_loss: 1.5427 - val_acc: 0.3221\n",
      "Epoch 55/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2673 - acc: 0.4747 - val_loss: 1.5200 - val_acc: 0.3289\n",
      "Epoch 56/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2447 - acc: 0.5051 - val_loss: 1.5166 - val_acc: 0.3423\n",
      "Epoch 57/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2756 - acc: 0.4662 - val_loss: 1.5135 - val_acc: 0.3356\n",
      "Epoch 58/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2700 - acc: 0.4611 - val_loss: 1.5087 - val_acc: 0.3490\n",
      "Epoch 59/100\n",
      "512/592 [========================>.....] - ETA: 0s - loss: 1.2128 - acc: 0.4902\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2117 - acc: 0.4882 - val_loss: 1.5127 - val_acc: 0.3423\n",
      "Epoch 60/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2609 - acc: 0.4696 - val_loss: 1.5112 - val_acc: 0.3490\n",
      "Epoch 61/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2512 - acc: 0.4983 - val_loss: 1.5081 - val_acc: 0.3490\n",
      "Epoch 62/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2211 - acc: 0.4949 - val_loss: 1.5097 - val_acc: 0.3557\n",
      "Epoch 63/100\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.2519 - acc: 0.4764 - val_loss: 1.5142 - val_acc: 0.3557\n",
      "Epoch 64/100\n",
      "512/592 [========================>.....] - ETA: 0s - loss: 1.2361 - acc: 0.4961\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2383 - acc: 0.4983 - val_loss: 1.5173 - val_acc: 0.3624\n",
      "Epoch 65/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2387 - acc: 0.4814 - val_loss: 1.5181 - val_acc: 0.3557\n",
      "Epoch 66/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2316 - acc: 0.4916 - val_loss: 1.5199 - val_acc: 0.3557\n",
      "Epoch 67/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2486 - acc: 0.4949 - val_loss: 1.5237 - val_acc: 0.3423\n",
      "Epoch 68/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2410 - acc: 0.4932 - val_loss: 1.5287 - val_acc: 0.3356\n",
      "Epoch 69/100\n",
      "512/592 [========================>.....] - ETA: 0s - loss: 1.2639 - acc: 0.4746\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2739 - acc: 0.4780 - val_loss: 1.5322 - val_acc: 0.3423\n",
      "Epoch 70/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2378 - acc: 0.5000 - val_loss: 1.5334 - val_acc: 0.3423\n",
      "Epoch 71/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2218 - acc: 0.5169 - val_loss: 1.5327 - val_acc: 0.3356\n",
      "Epoch 72/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2351 - acc: 0.4949 - val_loss: 1.5307 - val_acc: 0.3356\n",
      "Epoch 73/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2504 - acc: 0.4814 - val_loss: 1.5289 - val_acc: 0.3356\n",
      "Epoch 74/100\n",
      "512/592 [========================>.....] - ETA: 0s - loss: 1.2581 - acc: 0.4824\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2663 - acc: 0.4747 - val_loss: 1.5262 - val_acc: 0.3356\n",
      "Epoch 75/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2836 - acc: 0.4375 - val_loss: 1.5256 - val_acc: 0.3356\n",
      "Epoch 76/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2159 - acc: 0.4848 - val_loss: 1.5264 - val_acc: 0.3356\n",
      "Epoch 77/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2134 - acc: 0.4932 - val_loss: 1.5283 - val_acc: 0.3356\n",
      "Epoch 78/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2194 - acc: 0.4966 - val_loss: 1.5298 - val_acc: 0.3356\n",
      "Epoch 79/100\n",
      "512/592 [========================>.....] - ETA: 0s - loss: 1.2155 - acc: 0.4922\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2267 - acc: 0.4932 - val_loss: 1.5315 - val_acc: 0.3356\n",
      "Epoch 80/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2325 - acc: 0.5135 - val_loss: 1.5316 - val_acc: 0.3356\n",
      "Epoch 81/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2236 - acc: 0.4983 - val_loss: 1.5314 - val_acc: 0.3356\n",
      "Epoch 82/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2508 - acc: 0.5051 - val_loss: 1.5312 - val_acc: 0.3356\n",
      "Epoch 83/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2306 - acc: 0.5169 - val_loss: 1.5308 - val_acc: 0.3423\n",
      "Epoch 84/100\n",
      "512/592 [========================>.....] - ETA: 0s - loss: 1.2351 - acc: 0.4844\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2292 - acc: 0.4747 - val_loss: 1.5306 - val_acc: 0.3423\n",
      "Epoch 85/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2415 - acc: 0.5017 - val_loss: 1.5304 - val_acc: 0.3423\n",
      "Epoch 86/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2347 - acc: 0.5135 - val_loss: 1.5306 - val_acc: 0.3423\n",
      "Epoch 87/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2515 - acc: 0.4831 - val_loss: 1.5306 - val_acc: 0.3423\n",
      "Epoch 88/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2398 - acc: 0.4713 - val_loss: 1.5309 - val_acc: 0.3423\n",
      "Epoch 89/100\n",
      "512/592 [========================>.....] - ETA: 0s - loss: 1.2329 - acc: 0.4863\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2451 - acc: 0.4814 - val_loss: 1.5310 - val_acc: 0.3423\n",
      "Epoch 90/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2247 - acc: 0.4949 - val_loss: 1.5309 - val_acc: 0.3423\n",
      "Epoch 91/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2536 - acc: 0.4865 - val_loss: 1.5308 - val_acc: 0.3423\n",
      "Epoch 92/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2025 - acc: 0.5135 - val_loss: 1.5307 - val_acc: 0.3423\n",
      "Epoch 93/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2456 - acc: 0.4949 - val_loss: 1.5305 - val_acc: 0.3423\n",
      "Epoch 94/100\n",
      "512/592 [========================>.....] - ETA: 0s - loss: 1.1912 - acc: 0.5352\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.1892 - acc: 0.5355 - val_loss: 1.5306 - val_acc: 0.3423\n",
      "Epoch 95/100\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.2708 - acc: 0.4848 - val_loss: 1.5306 - val_acc: 0.3423\n",
      "Epoch 96/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2221 - acc: 0.5152 - val_loss: 1.5306 - val_acc: 0.3423\n",
      "Epoch 97/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2382 - acc: 0.5017 - val_loss: 1.5306 - val_acc: 0.3423\n",
      "Epoch 98/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2431 - acc: 0.5068 - val_loss: 1.5306 - val_acc: 0.3423\n",
      "Epoch 99/100\n",
      "512/592 [========================>.....] - ETA: 0s - loss: 1.2455 - acc: 0.4961\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2493 - acc: 0.4932 - val_loss: 1.5306 - val_acc: 0.3423\n",
      "Epoch 100/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.2514 - acc: 0.4764 - val_loss: 1.5306 - val_acc: 0.3423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd3d5004fd0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_SPLIT = 0.2\n",
    "\n",
    "EMBEDDING_DIM = 50  # 50 / 100 / 200 / 300\n",
    "MAX_VOCAB_SIZE = 6741\n",
    "MAX_SEQ_LENGTH = 611\n",
    "\n",
    "model_name_suffix = '-%d_dim_emb-%d_vocab' % (EMBEDDING_DIM, MAX_VOCAB_SIZE)\n",
    "\n",
    "datasetX_padded, tokenizer = process_dataset(lyrics_X, MAX_VOCAB_SIZE, MAX_SEQ_LENGTH)\n",
    "embedding_layer = get_embedding_layer(tokenizer, EMBEDDING_DIM, MAX_VOCAB_SIZE, MAX_SEQ_LENGTH)\n",
    "\n",
    "trainX_padded, testX_padded, trainY, testY = train_test_split(\n",
    "    datasetX_padded,\n",
    "    artist_onehot_Y,\n",
    "    test_size=TEST_SPLIT)\n",
    "\n",
    "sequence_input = kl.Input(shape=(MAX_SEQ_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = kl.SpatialDropout1D(0.2)(embedded_sequences)\n",
    "x = kl.LSTM(128)(x)\n",
    "x = kl.Dropout(0.5)(x)\n",
    "x = kl.Dense(128, activation='relu')(x)\n",
    "x = kl.Dropout(0.5)(x)\n",
    "preds = kl.Dense(len(BANDS), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds, name='1_LSTM_128-dense_128' + model_name_suffix)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    trainX_padded,\n",
    "    trainY,\n",
    "    validation_data=(testX_padded, testY),\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    callbacks = [\n",
    "        get_checkpoint_callback(model),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1),\n",
    "    ],\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th style=\"text-align:center\">Model</th>\n",
    "            <th>Training Loss</th>\n",
    "            <th>Validation Loss</th>\n",
    "            <th>Training Accuracy</th>\n",
    "            <th>Validation Accuracy</th>\n",
    "            <th>Overfitting</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"text-align:left\">\n",
    "                <pre>\n",
    "EMBEDDING_DIM = 50\n",
    "MAX_VOCAB_SIZE = 6741\n",
    "MAX_SEQ_LENGTH = 611\n",
    "Input -> Embedding -> SpatialDropout1D(0.2)\n",
    "-> LSTM(128) -> Dropout(0.5)\n",
    "-> Dense(128, 'relu') -> Dropout(0.5)\n",
    "-> Dense(len(BANDS), activation='softmax')\n",
    "                </pre>\n",
    "            </td>\n",
    "            <td style=\"text-align:center\">1.2514</td>\n",
    "            <td style=\"text-align:center\">1.5306</td>\n",
    "            <td style=\"text-align:center\">0.4764</td>\n",
    "            <td style=\"text-align:center\">0.3423</td>\n",
    "            <td style=\"text-align:center\">High bias,<br/>\n",
    "                High variance\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align:left\">\n",
    "                <pre>\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_VOCAB_SIZE = 6000\n",
    "MAX_SEQ_LENGTH = 600\n",
    "Input -> Embedding -> SpatialDropout1D(0.2)\n",
    "-> LSTM(128) -> Dropout(0.5)\n",
    "-> Dense(128, 'relu') -> Dropout(0.5)\n",
    "-> Dense(len(BANDS), activation='softmax')\n",
    "                </pre>\n",
    "            </td>\n",
    "            <td style=\"text-align:center\">1.1711</td>\n",
    "            <td style=\"text-align:center\">1.4861</td>\n",
    "            <td style=\"text-align:center\">0.5186</td>\n",
    "            <td style=\"text-align:center\">0.3624</td>\n",
    "            <td style=\"text-align:center\">High bias,<br/>\n",
    "                High variance\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align:left\">\n",
    "                <pre>\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_VOCAB_SIZE = 6000\n",
    "MAX_SEQ_LENGTH = 600\n",
    "Input -> Embedding -> SpatialDropout1D(0.2)\n",
    "-> Bidirectional_LSTM(128) -> Dropout(0.5)\n",
    "-> Dense(128, 'relu') -> Dropout(0.5)\n",
    "-> Dense(len(BANDS), activation='softmax')\n",
    "                </pre>\n",
    "            </td>\n",
    "            <td style=\"text-align:center\">1.1926</td>\n",
    "            <td style=\"text-align:center\">1.5276</td>\n",
    "            <td style=\"text-align:center\">0.5304</td>\n",
    "            <td style=\"text-align:center\">0.3490</td>\n",
    "            <td style=\"text-align:center\">High bias,<br/>\n",
    "                High variance\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align:left\">\n",
    "                <pre>\n",
    "EMBEDDING_DIM = 200\n",
    "MAX_VOCAB_SIZE = 6000\n",
    "MAX_SEQ_LENGTH = 600\n",
    "Input -> Embedding -> SpatialDropout1D(0.2)\n",
    "-> Bidirectional_LSTM(128) -> Dropout(0.5)\n",
    "-> Dense(128, 'relu') -> Dropout(0.5)\n",
    "-> Dense(len(BANDS), activation='softmax')\n",
    "                </pre>\n",
    "            </td>\n",
    "            <td style=\"text-align:center\">0.9014</td>\n",
    "            <td style=\"text-align:center\">1.6976</td>\n",
    "            <td style=\"text-align:center\">0.6689</td>\n",
    "            <td style=\"text-align:center\">0.3557</td>\n",
    "            <td style=\"text-align:center\">Definitely high variance,<br/>\n",
    "                could be high bias<br/>\n",
    "                (but cannot say for<br/>\n",
    "                sure as human performance<br/>\n",
    "                is not known for this problem)\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
